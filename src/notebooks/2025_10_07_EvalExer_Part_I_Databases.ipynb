{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st4fJtML5xID"
      },
      "source": [
        "# Evaluated Exercise - Part 1: Databases\n",
        "\n",
        "Please upload an html-version of your final version into the drop-zone in Moodle. If you have any issues with it, send the final version to guido.moeser@gmail.com.   \n",
        "Release Date: 2025-10-07\n",
        "\n",
        "## Final Submission Instructions\n",
        "\n",
        "1. Complete all sections in the notebook.\n",
        "2. Add explanations of all parts. **Explanations are the most important part for the grading.**\n",
        "3. Comment on which configuration you found best and why. **Comments are the second most important part for the grading.**\n",
        "4. Export your Jupyter Notebook to HTML and send the HTML-version.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxVuHeJB6QAy"
      },
      "source": [
        "# Topic: Building a simple Retrieval-Augmented Retrieval System with SQLite (in-memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvruTfg86TVg"
      },
      "source": [
        "## 1 – Import the required packages\n",
        "\n",
        "**Required packages etc:**\n",
        "- sqlite3\n",
        "- pandas\n",
        "- numpy\n",
        "- from sklearn.metrics.pairwise we need cosine_similarity\n",
        "- from the SentenceTransformer package we need the SentenceTransformer class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J3Ad435O5wcY"
      },
      "outputs": [],
      "source": [
        "# Load the minimal set of packages we will need\n",
        "# Uncomment below lines to install if using pixi package manager\n",
        "#%pixi add pandas, numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoFe4yVt6WND"
      },
      "source": [
        "**Explanation:**\n",
        "- We will only use the core Python SQLite library and three data-science packages: pandas, numpy, and sentence-transformers.\n",
        "- No extra dependencies are needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcmAqaRK6bPm"
      },
      "source": [
        "## 2 – Provide your own texts\n",
        "\n",
        "Please insert 10 texts of length > 200 words (news articles, wikipedia article (parts of it), product descriptions, product reviews etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9-Bn4CN6Vhz"
      },
      "outputs": [],
      "source": [
        "# Replace these sample texts with 10 texts of your own choice, length > 200 word.\n",
        "texts = [\n",
        "    \"Text 1 – replace me with your own text.\",\n",
        "    \"Text 2 – replace me with your own text.\",\n",
        "    # ...\n",
        "    \"Text 10 – replace me with your own text.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fxlqsSg6mrY"
      },
      "source": [
        "## 3 – Load the SentenceTransformer model\n",
        "\n",
        "**Tasks:**\n",
        "- Load the `all-MiniLM-L6-v2` using `SentenceTransformer()` and apply it to a sentence of your choice to show the embeddings.\n",
        "- Explain what the embeddings are on a short sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-116cy86drF"
      },
      "outputs": [],
      "source": [
        "# Load a lightweight embedding model from Huggingface\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emBFhElX6uVl"
      },
      "source": [
        "**Explanation:** ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Fidel's answer:** <br>\n",
        "Embeddings are numerical column vector representation of words, sentences, paragraphs, etc. In order to achieve vector representation of the words, sentences, or paragraphs; these have to be split into **tokens** and each token is converted to a numerical value and these numeric values are grouped togheter on the size of the **context window**; the context window is choosen based on many approaches(similarity, context, word reelevance, etc.). Ultimately, these numbers are put in a single column vector which is the vector that embeds a representation of the word, sentences, or paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlW2XevJ68mv"
      },
      "source": [
        "## 4 – Create and fill an in-memory SQLite database\n",
        "\n",
        "- Create an in-memory database with SQLite\n",
        "- Create a table with the fields\n",
        "  - id,\n",
        "  - title,\n",
        "  - text,\n",
        "  - embedding,\n",
        "  - import_time\n",
        "- Metadata: Just add the time you loaded the data into the database\n",
        "- Load the data into the database: `INSERT INTO documents (title, text, embedding, import_time) VALUES (?, ?, ?, ?)`\n",
        "- Use pandas to run a SQL-request against the table to show that everything works fine\n",
        "\n",
        "**Task:** Add the necessary SQL query. All other parts are already there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t4IuIdO6tfA"
      },
      "outputs": [],
      "source": [
        "# Create an in-memory database\n",
        "conn = sqlite3.connect(\":memory:\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create a simple table (embedding stored as TEXT)\n",
        "cursor.execute(\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Insert documents with metadata\n",
        "import datetime\n",
        "\n",
        "for i, t in enumerate(texts):\n",
        "    title = f\"Doc_{i+1}\"\n",
        "    emb = model.encode([t])[0]\n",
        "    emb_str = \",\".join(map(str, emb))   # convert vector to comma-separated string\n",
        "    cursor.execute(\n",
        "        \"INSERT INTO documents (title, text, embedding, import_time) VALUES (?, ?, ?, ?)\",\n",
        "        (title, t, emb_str, str(datetime.datetime.now()))\n",
        "    )\n",
        "\n",
        "conn.commit()\n",
        "\n",
        "# Test query using pandas\n",
        "pd.read_sql(\"SELECT id, title, import_time FROM documents\", conn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miWwnRsy8SPA"
      },
      "source": [
        "## 5 – Define and test three similarity metrics\n",
        "\n",
        "- **Task:** Please build to more functions. The functions should return a similarity score between two vectors.\n",
        "- Will be used later to compare which metric retrieves the most relevant texts.\n",
        "\n",
        "Here is one function:\n",
        "\n",
        "```\n",
        "# Metric 1: cosine similarity (from scikit-learn)\n",
        "def cosine_sim(a, b):\n",
        "    return cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0][0]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7hW0WS7r2U"
      },
      "outputs": [],
      "source": [
        "# Metric 1: cosine similarity (from scikit-learn)\n",
        "def cosine_sim(a, b):\n",
        "    return cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0][0]\n",
        "\n",
        "# Metric 2:\n",
        "def\n",
        "\n",
        "\n",
        "# Metric 3:\n",
        "def\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpE3vv-j8Xke"
      },
      "source": [
        "## 6 – Build a simple Retriever\n",
        "\n",
        "A function that encodes a query, computes the similarity between the query and each document embedding, and returns the top 3 most similar texts by default.  \n",
        "  \n",
        "**Task: Code is complete, please explain what happens here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD_FaJJh8q91"
      },
      "outputs": [],
      "source": [
        "def retrieve(query, metric_function, top_k=3):\n",
        "    # Encode the query\n",
        "    q_vec = model.encode([query])[0]\n",
        "\n",
        "    # Load all document embeddings\n",
        "    cursor.execute(\"SELECT id, title, text, embedding FROM documents\")\n",
        "    docs = cursor.fetchall()\n",
        "\n",
        "    results = []\n",
        "    for doc_id, title, text, emb_str in docs:\n",
        "        emb = np.fromstring(emb_str, sep=\",\")     # convert text back to numeric vector\n",
        "        score = metric_function(q_vec, emb)\n",
        "        results.append((title, text, score))\n",
        "\n",
        "    # Sort by similarity and return top_k results\n",
        "    results = sorted(results, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "    return pd.DataFrame(results, columns=[\"Title\", \"Text\", \"Score\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNSJvgfcQsA-"
      },
      "source": [
        "**Explanation**\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsWTRw0Z_BB0"
      },
      "source": [
        "## 7 – Run a query and inspect the results\n",
        "\n",
        "- Run the retriever with one of the metrics (cosine_sim, dot_product_sim, or inv_euclidean_sim).\n",
        "- Check how the ranking of results changes across metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqrWCS8m8slI"
      },
      "outputs": [],
      "source": [
        "query = \"Enter your own test question here\"\n",
        "retrieve(query, cosine_sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCZeANDC_H5X"
      },
      "source": [
        "# Fine-Tuning Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwDVaUje_LDt"
      },
      "source": [
        "## 8 – Reload database with different chunk sizes and overlaps\n",
        "\n",
        "- Reload your in-memory database with various chunk_size and overlap settings (e.g. 30/10, 60/20).\n",
        "- For each configuration, insert each chunk as a new row in the documents table and repeat the insertion logic from Section 4 (using comma-separated embeddings).\n",
        "\n",
        "**Task:** Explain what happens here and test the functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7sLualE_JW2"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=50, overlap=10):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlPMlFjdQ9Fp"
      },
      "source": [
        "**Explanations:** ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFHMT4pUQ-7_"
      },
      "outputs": [],
      "source": [
        "# Test the function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZT7Rhcj_StG"
      },
      "source": [
        "## 9 – Query the new database with different similarity metrics\n",
        "\n",
        "Compare retrieval quality under the different metrics developed above. A record which combination of metric, chunk size, and overlap yields the most meaningful matches will be printed out.\n",
        "\n",
        "**Task:** Replace the similarity-functions here with the functions you developed above:\n",
        "\n",
        "```\n",
        "for metric in [cosine_sim, <your sim function>, <your sim function>]:\n",
        "    print(f\"Results using {metric.__name__}:\")\n",
        "    display(retrieve(query, metric))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WputV4ZL_Mpl"
      },
      "outputs": [],
      "source": [
        "for metric in [cosine_sim, dot_product_sim, inv_euclidean_sim]:\n",
        "    print(f\"Results using {metric.__name__}:\")\n",
        "    display(retrieve(query, metric))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3onl0c-n_Zbq"
      },
      "source": [
        "## 10 – Use the systematic evaluation module\n",
        "\n",
        "- This module systematically tests different configurations (chunk size, overlap, metric) and records which text was ranked highest.\n",
        "- Visualize or summarize the outcomes to decide which configuration works best.\n",
        "\n",
        "**Task:** Explain what happens here and run the experiment with different settings for\n",
        "- chunksize\n",
        "- overlap\n",
        "- similarity functions\n",
        "\n",
        "(Modify the experiment if you want, but not necessary or required).\n",
        "\n",
        "*Please note: Replace the similarity functions with your similarity functions, otherwise it will throw an error.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1a2X8kA_UZa"
      },
      "outputs": [],
      "source": [
        "def evaluate_configs(query, chunk_sizes, overlaps, metrics):\n",
        "    results = []\n",
        "\n",
        "    for cs in chunk_sizes:\n",
        "        for ov in overlaps:\n",
        "            # Rebuild an in-memory DB for each configuration\n",
        "            conn = sqlite3.connect(\":memory:\")\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"CREATE TABLE docs (id INTEGER PRIMARY KEY, text TEXT, embedding TEXT)\")\n",
        "\n",
        "            # Insert chunks with textual embeddings\n",
        "            for t in texts:\n",
        "                for ch in chunk_text(t, cs, ov):\n",
        "                    emb = model.encode([ch])[0]\n",
        "                    emb_str = \",\".join(map(str, emb))\n",
        "                    cursor.execute(\"INSERT INTO docs (text, embedding) VALUES (?, ?)\", (ch, emb_str))\n",
        "            conn.commit()\n",
        "\n",
        "            # Encode the query once\n",
        "            q_vec = model.encode([query])[0]\n",
        "\n",
        "            # Evaluate all metrics\n",
        "            for metric in metrics:\n",
        "                cursor.execute(\"SELECT id, text, embedding FROM docs\")\n",
        "                docs = cursor.fetchall()\n",
        "                scores = []\n",
        "\n",
        "                for _, text, emb_str in docs:\n",
        "                    emb = np.fromstring(emb_str, sep=\",\")\n",
        "                    score = metric(q_vec, emb)\n",
        "                    scores.append((text, score))\n",
        "\n",
        "                # Sort by similarity and take the best one\n",
        "                top_text, top_score = sorted(scores, key=lambda x: x[1], reverse=True)[0]\n",
        "\n",
        "                results.append((cs, ov, metric.__name__, top_text, top_score))\n",
        "\n",
        "    # Return results as DataFrame\n",
        "    return pd.DataFrame(results, columns=[\"ChunkSize\", \"Overlap\", \"Metric\", \"TopResult\", \"SimilarityScore\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06SmnkMNRiyY"
      },
      "source": [
        "**Explanations:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnA-aMAh_rL4"
      },
      "source": [
        "**Example usage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcBkU-Oq_qfp"
      },
      "outputs": [],
      "source": [
        "# Define your test query\n",
        "query = \"Summarize the main idea of my texts\"\n",
        "\n",
        "# Define parameter grid\n",
        "chunk_sizes = [30, 50, 70]\n",
        "overlaps = [5, 10, 15]\n",
        "metrics = [cosine_sim, dot_product_sim, inv_euclidean_sim]\n",
        "\n",
        "# Run evaluation\n",
        "results_df = evaluate_configs(query, chunk_sizes, overlaps, metrics)\n",
        "\n",
        "# Display full table\n",
        "display(results_df)\n",
        "\n",
        "# Optional: Find the highest similarity overall\n",
        "best_config = results_df.sort_values(\"SimilarityScore\", ascending=False).head(1)\n",
        "display(best_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkvMzJjQR5-s"
      },
      "source": [
        "**Interpretation of Results** ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "default",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
